{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['credentials.yml',\n",
       " 'downloded_data',\n",
       " 'generated_Tensors',\n",
       " 'train_labels.csv',\n",
       " 'train_metadata.csv',\n",
       " 'video']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "import pandas as pd\n",
    "from boto3.session import Session\n",
    "import cv2\n",
    "import numpy as np\n",
    "import ipyvolume as ipv\n",
    "import joblib # joblib version: 0.9.4\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "config = './config.yml'\n",
    "with open (config , 'rb') as f:\n",
    "    config = yaml.load(f ,Loader=yaml.FullLoader)\n",
    "    \n",
    "os.listdir(config['dataset']['path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClogLossDataset_downloader:\n",
    "    def __init__(self, config , online_data= True , draw_3d = False ):\n",
    "        self.cfg = config\n",
    "        self.dataPath = config['dataset']['path']\n",
    "        self.videoPath = os.path.join(config['dataset']['path'], 'video')\n",
    "        self.online_data = online_data\n",
    "        self.remove = config['dataset']['remove_donloaded_video']\n",
    "        self.draw_3d = draw_3d\n",
    "        self.saveDatasetDir  = config['dataset']['save_dir']\n",
    "        \n",
    "        if not os.path.exists(self.saveDatasetDir):\n",
    "            os.makedirs(self.saveDatasetDir)\n",
    "        \n",
    "        metaData = os.path.join(self.dataPath ,'train_metadata.csv')\n",
    "        metaData = pd.read_csv(metaData)\n",
    "        \n",
    "        label = os.path.join(self.dataPath ,'train_labels.csv')\n",
    "        label = pd.read_csv(label)\n",
    "        \n",
    "        self.df_dataset = metaData\n",
    "        self.df_dataset['stalled'] =label['stalled']\n",
    "        \n",
    "#         self.df_dataset = metaData[metaData['filename'].isin(df['filename'])]\n",
    "#         self.df_dataset['stalled'] =label[label['filename'].isin(df['filename'])]['stalled']\n",
    "        self.df_dataset['vid_id'] = self.df_dataset.index\n",
    "        \n",
    "        \n",
    "        if True:\n",
    "            self.download_fldr = 'downloded_data' \n",
    "            self.download_fldr = os.path.join(self.dataPath ,self.download_fldr )\n",
    "            if not os.path.exists(f\"./{self.download_fldr}\"):\n",
    "                os.mkdir(self.saveDatasetDir)\n",
    "            credentials_path = config['dataset']['credentials_path']\n",
    "            with open (credentials_path , 'rb') as f:\n",
    "                credentials = yaml.load(f ,Loader=yaml.FullLoader)\n",
    "#                 print(credentials)\n",
    "\n",
    "            ACCESS_KEY = credentials['ACCESS_KEY']\n",
    "            SECRET_KEY = credentials['SECRET_KEY']\n",
    "\n",
    "            session = Session(aws_access_key_id=ACCESS_KEY,\n",
    "                          aws_secret_access_key=SECRET_KEY)\n",
    "            s3 = session.resource('s3')\n",
    "            self.bucket = s3.Bucket('drivendata-competition-clog-loss')\n",
    "#             self.df_dataset = self.df_dataset[self.df_dataset['num_frames'] > 200]\n",
    "#             train_Dataset.df_dataset[train_Dataset.df_dataset['tier1']== True]\n",
    "            \n",
    "#             self.df_dataset = self.df_dataset[self.df_dataset['stalled']==0]\n",
    "\n",
    "#             for s3_file in your_bucket.objects.all():\n",
    "#                 print(s3_file.key) # prints the contents of bucket\n",
    "                \n",
    "        else:\n",
    "            df = pd.DataFrame([file for file in os.listdir(self.videoPath)  if file.split('.')[-1] == 'mp4'], columns=['filename'])\n",
    "            self.df_dataset = self.df_dataset[metaData['filename'].isin(df['filename'])]\n",
    "            self.df_dataset = self.df_dataset.reset_index(drop = True)\n",
    "            \n",
    "        self.number_of_objec = len(self.df_dataset)\n",
    "        self.current_row=0\n",
    "        \n",
    "    def getFrame( self , vidcap , sec , image_name ):\n",
    "        vidcap.set(cv2.CAP_PROP_POS_MSEC, sec * 1000)\n",
    "        hasFrames,image = vidcap.read()\n",
    "        if(hasFrames):\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        return image ,hasFrames\n",
    "    \n",
    "    def get_specified_area(self , image):\n",
    "    \n",
    "        # convert to hsv to detect the outlined orange area\n",
    "        hsv = cv2.cvtColor(image,cv2.COLOR_BGR2HSV)\n",
    "        lower_red = np.array([100,120,150])\n",
    "        upper_red = np.array([110,255,255])\n",
    "        # create a mask\n",
    "        mask1 = cv2.inRange(hsv, lower_red, upper_red)\n",
    "        mask1 = cv2.dilate(mask1, None, iterations=2)\n",
    "        mask_ind = np.where(mask1>0)\n",
    "        xmin , xmax = min(mask_ind[1]) , max(mask_ind[1])\n",
    "        ymin , ymax = min(mask_ind[0]) , max(mask_ind[0])\n",
    "        # remove orange line from the image\n",
    "        return mask1 ,(xmin , xmax , ymin , ymax)\n",
    "    \n",
    "    \n",
    "    def filter_image(self, image ,mask1 ,area):\n",
    "        xmin , xmax,ymin , ymax = area\n",
    "        \n",
    "        mask_ind = np.where(mask1>0)\n",
    "        image[mask_ind ]=0,0,0\n",
    "        # fill the area to skip the data outside of this area\n",
    "        ret,mask1 = cv2.threshold(mask1,10,255,cv2.THRESH_BINARY_INV)\n",
    "        contours,hierarchy = cv2.findContours(mask1, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "        contours = [ctr for ctr in contours if cv2.contourArea(ctr) < 5*(mask1.shape[0]*mask1.shape[1])/6]\n",
    "        contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "#         print(len(contours))\n",
    "        cv2.drawContours(mask1, [contours[-1]], -1, (0, 0, 0), -1)\n",
    "        # remove data out of the outlined area\n",
    "        image[mask1>0] = (0,0,0)\n",
    "        \n",
    "    #     image =  cv2.rectangle(image , (xmin,ymin) ,(xmax,ymax),(255,255,255),4,4)\n",
    "        image = image[ ymin:ymax , xmin:xmax ]\n",
    "        image = cv2.resize(image ,(150,150))\n",
    "        \n",
    "#         image = image /255.\n",
    "    #     image -= image.mean()\n",
    "    #     image /= image.std()\n",
    "    #     print(image.shape , xmin , xmax,ymin , ymax)\n",
    "        return image\n",
    "    \n",
    "    @staticmethod\n",
    "    def draw_tensor(tensor_img):\n",
    "\n",
    "        ipv.figure()\n",
    "        ipv.volshow(tensor_img[...,0], level=[0.36, 0.55,1], opacity=[0.11,0.13, 0.13], level_width=0.05, data_min=0, data_max=1 ,lighting=True)\n",
    "        ipv.view(-30, 45)\n",
    "        ipv.show()\n",
    "        \n",
    "    def create_metadata(self,row):\n",
    "        meta_data = {}\n",
    "        meta_data['filename'] = row.filename\n",
    "        meta_data['crowd_score'] = row.crowd_score\n",
    "        meta_data['tier1'] = row.tier1\n",
    "        meta_data['stalled'] = row.stalled\n",
    "        meta_data['vid_id'] = row.vid_id\n",
    "        meta_data['project_id'] = row.project_id\n",
    "        meta_data['num_frames'] = row.num_frames\n",
    "        return meta_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df_dataset)-1\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df_dataset.iloc[index]\n",
    "        \n",
    "        metadata = self.create_metadata(row)\n",
    "        \n",
    "        if self.online_data:\n",
    "            vid_p = os.path.join(self.download_fldr ,f\"{row.filename}\")\n",
    "            self.bucket.download_file(f\"train/{row.filename}\",vid_p )       \n",
    "            vidcap = cv2.VideoCapture(vid_p)\n",
    "#             \n",
    "        else:\n",
    "            vidcap = cv2.VideoCapture(os.path.join(self.videoPath,row.filename))\n",
    "        total_frames = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "#         total_frames = config['dataset']['num_frames']\n",
    "        frame_size = (int(vidcap.get(cv2.CAP_PROP_FRAME_WIDTH)) , int(vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT )))\n",
    "        fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "        Video_len = total_frames / fps\n",
    "        from_sec = 0 \n",
    "        time_stamp = np.linspace(from_sec , Video_len , int(total_frames / 1.0) )\n",
    "    \n",
    "        tensor_img = []\n",
    "        \n",
    "        for frame in range(int(total_frames)):\n",
    "            image , hasframe = self.getFrame(vidcap ,time_stamp[frame] , frame)\n",
    "            \n",
    "            if hasframe:\n",
    "                if frame==0:\n",
    "                    mask , area = self.get_specified_area(image)\n",
    "                image = self.filter_image(image , mask, area)\n",
    "                tensor_img.append(image)\n",
    "                \n",
    "            if frame >= 199:\n",
    "                break\n",
    "            \n",
    "            \n",
    "        if  len(tensor_img) < 200:\n",
    "            for kk in range(200 - len(tensor_img) ):\n",
    "                tensor_img.append(list(np.zeros([150,150,3])))\n",
    "                \n",
    "#         print(len(tensor_img))\n",
    "        vidcap.release()  \n",
    "        if self.remove:\n",
    "            os.remove(vid_p)\n",
    "        tensor_img = np.array(list(tensor_img))\n",
    "#         print(tensor_img.shape)\n",
    "        if self.draw_3d :\n",
    "            self.draw_tensor(tensor_img)\n",
    "#         print(row)\n",
    "#         tensor_img = np.moveaxis(tensor_img,3,0)\n",
    "        tensor_img = tensor_img.astype(np.uint8)\n",
    "        joblib.dump([tensor_img , metadata], os.path.join(self.saveDatasetDir ,f\"{row.filename.split('.')[0]}.lzma\"), compress=('lzma', 6))\n",
    "#         print(os.path.join(self.saveDatasetDir ,f\"{row.filename}\"))\n",
    "        \n",
    "        return [tensor_img , metadata]\n",
    "            \n",
    "            \n",
    "#         def __iter__(self):\n",
    "#             return self\n",
    "\n",
    "#         def __next__(self): # Python 2: def next(self)\n",
    "#             row = self.df_dataset.iloc[self.current_row]\n",
    "#             self.current_row +=1\n",
    "#             if self.current_row < self.number_of_objec:\n",
    "#                 return row\n",
    "#             raise StopIteration\n",
    "            \n",
    "            \n",
    "video_downloder = ClogLossDataset_downloader(config  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "573047\n"
     ]
    }
   ],
   "source": [
    "print(len(video_downloder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5528d26ee58841dea99066d495180223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=573047.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..\\\\..\\\\data\\\\generated_Tensors\\\\100000.lzma'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-55a38f8eb3aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_downloder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mvideo_downloder\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-68-8a29be270816>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;31m#         tensor_img = np.moveaxis(tensor_img,3,0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[0mtensor_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_img\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensor_img\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveDatasetDir\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;34mf\"{row.filename.split('.')[0]}.lzma\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lzma'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m \u001b[1;31m#         print(os.path.join(self.saveDatasetDir ,f\"{row.filename}\"))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py37\\lib\\site-packages\\joblib\\numpy_pickle.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompress_level\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m         with _write_fileobject(filename, compress=(compress_method,\n\u001b[1;32m--> 501\u001b[1;33m                                                    compress_level)) as f:\n\u001b[0m\u001b[0;32m    502\u001b[0m             \u001b[0mNumpyPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mis_filename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py37\\lib\\site-packages\\joblib\\numpy_pickle_utils.py\u001b[0m in \u001b[0;36m_write_fileobject\u001b[1;34m(filename, compress)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompressmethod\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_COMPRESSORS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         file_instance = _COMPRESSORS[compressmethod].compressor_file(\n\u001b[1;32m--> 192\u001b[1;33m             filename, compresslevel=compresslevel)\n\u001b[0m\u001b[0;32m    193\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_buffered_write_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_instance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py37\\lib\\site-packages\\joblib\\compressor.py\u001b[0m in \u001b[0;36mcompressor_file\u001b[1;34m(self, fileobj, compresslevel)\u001b[0m\n\u001b[0;32m    172\u001b[0m             return self.fileobj_factory(fileobj, 'wb',\n\u001b[0;32m    173\u001b[0m                                         \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlzma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFORMAT_ALONE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m                                         preset=compresslevel)\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecompressor_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py37\\lib\\lzma.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename, mode, format, check, preset, filters)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[0mmode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"b\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_closefp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode_code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '..\\\\..\\\\data\\\\generated_Tensors\\\\100000.lzma'"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(video_downloder))):\n",
    "    video_downloder[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_downloder[573047]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.load('../../data/generated_Tensors/100010.lzma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# def getFrame(  vidcap , sec , image_name ):\n",
    "#     vidcap.set(cv2.CAP_PROP_POS_MSEC, sec * 1000)\n",
    "#     hasFrames,image = vidcap.read()\n",
    "#     if(hasFrames):\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#     return image ,hasFrames\n",
    "\n",
    "\n",
    "# vidcap = cv2.VideoCapture('./test2.avi')\n",
    "# total_frames = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "# #         total_frames = config['dataset']['num_frames']\n",
    "# frame_size = (int(vidcap.get(cv2.CAP_PROP_FRAME_WIDTH)) , int(vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT )))\n",
    "# fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "# Video_len = total_frames / fps\n",
    "# from_sec = 0 \n",
    "# time_stamp = np.linspace(from_sec , Video_len , int(total_frames+1 / 1.0) )\n",
    "\n",
    "# tensor_img = []\n",
    "\n",
    "# for frame in range(int(total_frames)):\n",
    "#     image , hasframe = getFrame(vidcap ,time_stamp[frame] , frame)\n",
    "\n",
    "#     if hasframe:\n",
    "#         tensor_img.append(image)\n",
    "        \n",
    "# b = np.array(tensor_img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# class Counter:\n",
    "#     def __init__(self, low, high):\n",
    "#         self.current = low - 1\n",
    "#         self.high = high\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         return self\n",
    "\n",
    "#     def __next__(self): # Python 2: def next(self)\n",
    "#         self.current += 1\n",
    "#         if self.current < self.high:\n",
    "#             return self.current\n",
    "#         raise StopIteration\n",
    "\n",
    "\n",
    "# for c in Counter(3, 9):\n",
    "#     print(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
