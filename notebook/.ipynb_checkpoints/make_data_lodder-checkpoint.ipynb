{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_labels.csv', 'train_metadata.csv', 'video']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "\n",
    "config = './config.yml'\n",
    "with open (config , 'rb') as f:\n",
    "    config = yaml.load(f ,Loader=yaml.FullLoader)\n",
    "    \n",
    "os.listdir(config['dataset']['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        filename                                                url  \\\n",
      "0     100109.mp4  s3://drivendata-competition-clog-loss/train/10...   \n",
      "1     100289.mp4  s3://drivendata-competition-clog-loss/train/10...   \n",
      "2     100588.mp4  s3://drivendata-competition-clog-loss/train/10...   \n",
      "3     100750.mp4  s3://drivendata-competition-clog-loss/train/10...   \n",
      "4     102393.mp4  s3://drivendata-competition-clog-loss/train/10...   \n",
      "...          ...                                                ...   \n",
      "1408  684243.mp4  s3://drivendata-competition-clog-loss/train/68...   \n",
      "1409  684329.mp4  s3://drivendata-competition-clog-loss/train/68...   \n",
      "1410  684600.mp4  s3://drivendata-competition-clog-loss/train/68...   \n",
      "1411  684744.mp4  s3://drivendata-competition-clog-loss/train/68...   \n",
      "1412  685597.mp4  s3://drivendata-competition-clog-loss/train/68...   \n",
      "\n",
      "     project_id  num_frames  crowd_score  tier1  micro  nano  vid_id  \n",
      "0             L          67     0.000000   True   True  True     107  \n",
      "1             G          59     0.765824   True   True  True     280  \n",
      "2             A          53     0.000000   True   True  True     571  \n",
      "3             L          63     0.000000   True   True  True     730  \n",
      "4             K          66     0.000000   True   True  True    2328  \n",
      "...         ...         ...          ...    ...    ...   ...     ...  \n",
      "1408          F          55     0.764303   True   True  True  570156  \n",
      "1409          L          52     0.000000   True   True  True  570239  \n",
      "1410          A          49     1.000000   True   True  True  570501  \n",
      "1411          G          53     0.000000   True   True  True  570636  \n",
      "1412          K          48     0.000000   True   True  True  571469  \n",
      "\n",
      "[1413 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alibh\\Anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "class ClogLossDataset(Dataset):\n",
    "    def __init__(self, config , split = 'train' , type ='train' ):\n",
    "        self.cfg = config\n",
    "        self.dataPath = config['dataset']['path']\n",
    "        videoPath = os.path.join(config['dataset']['path'], 'video')\n",
    "        df = pd.DataFrame([file for file in os.listdir(videoPath)  if file.split('.')[-1] == 'mp4'], columns=['filename'])\n",
    "        metaData = os.path.join(self.dataPath ,'train_metadata.csv')\n",
    "        metaData = pd.read_csv(metaData)\n",
    "        \n",
    "        self.df_dataset = metaData[metaData['filename'].isin(df['filename'])]\n",
    "        self.df_dataset['vid_id'] = self.df_dataset.index\n",
    "        self.df_dataset = self.df_dataset.reset_index(drop = True)\n",
    "#         self.df_dataset['num_frames'].plot.hist()\n",
    "        \n",
    "        print((self.df_dataset))\n",
    "        \n",
    "    def getFrame( self , vidcap , sec , image_name ):\n",
    "        vidcap.set(cv2.CAP_PROP_POS_MSEC, sec * 1000)\n",
    "        hasFrames,image = vidcap.read()\n",
    "        if(hasFrames):\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        return image ,hasFrames\n",
    "    \n",
    "    def get_specified_area(self , image):\n",
    "    \n",
    "        # convert to hsv to detect the outlined orange area\n",
    "        hsv = cv2.cvtColor(image,cv2.COLOR_BGR2HSV)\n",
    "        lower_red = np.array([100,120,150])\n",
    "        upper_red = np.array([110,255,255])\n",
    "        # create a mask\n",
    "        mask1 = cv2.inRange(hsv, lower_red, upper_red)\n",
    "        mask1 = cv2.dilate(mask1, None, iterations=2)\n",
    "        mask_ind = np.where(mask1>0)\n",
    "        xmin , xmax = min(mask_ind[1]) , max(mask_ind[1])\n",
    "        ymin , ymax = min(mask_ind[0]) , max(mask_ind[0])\n",
    "        # remove orange line from the image\n",
    "        image[mask_ind ]=0,0,0\n",
    "        # fill the area to skip the data outside of this area\n",
    "        ret,mask1 = cv2.threshold(mask1,10,255,cv2.THRESH_BINARY_INV)\n",
    "        contours,hierarchy = cv2.findContours(mask1, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "        contours = [ctr for ctr in contours if cv2.contourArea(ctr) < 5*(mask1.shape[0]*mask1.shape[1])/6]\n",
    "        contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "        cv2.drawContours(mask1, [contours[1]], -1, (0, 0, 0), -1)\n",
    "        # remove data out of the outlined area\n",
    "        image[mask1>0] = (0,0,0)\n",
    "\n",
    "        return image ,(xmin , xmax , ymin , ymax)\n",
    "\n",
    "    def filter_image(self, image ,area):\n",
    "        xmin , xmax,ymin , ymax = area\n",
    "    #     image =  cv2.rectangle(image , (xmin,ymin) ,(xmax,ymax),(255,255,255),4,4)\n",
    "        image = image[ ymin:ymax , xmin:xmax ]\n",
    "        image = cv2.resize(image ,(150,150))\n",
    "        image = image /255.\n",
    "    #     image -= image.mean()\n",
    "    #     image /= image.std()\n",
    "    #     print(image.shape , xmin , xmax,ymin , ymax)\n",
    "        return image\n",
    "    \n",
    "    def draw_tensor(self, tensor_img):\n",
    "\n",
    "        ipv.figure()\n",
    "        ipv.volshow(tensor_img[...,2], level=[0.41, 0.75], opacity=0.5, level_width=0.1, data_min=0, data_max=1)\n",
    "        ipv.view(-30, 40)\n",
    "        ipv.show()\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df_dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df_dataset.iloc[index]\n",
    "        vidcap = cv2.VideoCapture(os.path.join(self.dataPath,'video',row.filename))\n",
    "        total_frames = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "#         total_frames = config['dataset']['num_frames']\n",
    "        frame_size = (int(vidcap.get(cv2.CAP_PROP_FRAME_WIDTH)) , int(vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT )))\n",
    "        fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "        Video_len = total_frames / fps\n",
    "        time_stamp = np.linspace(from_sec , Video_len , int(total_frames / step) )\n",
    "        sec = 0 \n",
    "        tensor_img = []\n",
    "        for frame in range(int(total_frames)):\n",
    "            image , hasframe = self.getFrame(vidcap ,time_stamp[frame] , frame)\n",
    "            if hasframe:\n",
    "                if frame==0:\n",
    "                    image , area = self.get_specified_area(image)\n",
    "            else:\n",
    "                image , _ = self.get_specified_area(image)\n",
    "            image = self.filter_image(image , area)\n",
    "            tensor_img.append(image)\n",
    "        image = filter_image(image , area)\n",
    "        tensor_img = np.array(list(tensor_img))\n",
    "        \n",
    "        self.draw_tensor(tensor_img)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "train_Dataset = ClogLossDataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataPath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-f4d665d8acce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_Dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-57-5b29ff03322e>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mvid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mvidcap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataPath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'video'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[0mtotal_frames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvidcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCAP_PROP_FRAME_COUNT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;31m#         total_frames = config['dataset']['num_frames']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataPath' is not defined"
     ]
    }
   ],
   "source": [
    "train_Dataset[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
